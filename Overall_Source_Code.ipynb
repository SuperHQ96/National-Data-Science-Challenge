{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overall Source Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H4G21nBTOBD8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Description"
      ]
    },
    {
      "metadata": {
        "id": "3fKNPisQzQm4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This source code makes use of Google Colab and Google Drive. We first attempted to create a logistic regression model for text classification which gave us the highest overall accuracy. Afterwards, we attempted to come up with a visual model using Pytorch but only managed to obtain a very low accuracy of 0.2. Finally, we decided to classify the test dataset into the three broad categories of mobile, beauty and fashion first before making use of three separate text classification model for each of the categories to further classify them into the sub-categories. However, it was slightly less accurate than the linear text classification model."
      ]
    },
    {
      "metadata": {
        "id": "ScKyQjXyzfLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VOTlS1myOwnh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Text Model"
      ]
    },
    {
      "metadata": {
        "id": "8gwMeOVnLitn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas as pd, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OO4s9CaDPEtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainDF = pd.read_csv('C:/Users/alyta/Downloads/year 2/year 2 sem 2/NDSC/dataset/train.csv')\n",
        "col = ['title', 'Category']\n",
        "trainDF = trainDF[col]\n",
        "\n",
        "# trainDFF = trainDF[]\n",
        "print(trainDF.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-DX2SREHPJsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read test data\n",
        "test = pd.read_csv('C:/Users/alyta/Downloads/year 2/year 2 sem 2/NDSC/dataset/test.csv')\n",
        "print(test.shape)\n",
        "test_x=test[\"title\"]\n",
        "\n",
        "#prediction\n",
        "results = pd.DataFrame(columns = [\"itemid\", \"Category\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z11oGxo6PLHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nltk.download('stopwords')\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "\n",
        "#use porter stemmer\n",
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "  \n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "#remove digits\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def normalize_corpus(corpus):\n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:   \n",
        "        #if text_lower_case:\n",
        "        doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        #if text_lemmatization:\n",
        "         #   doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        #if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "        doc=simple_stemmer(doc)\n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        doc = remove_special_characters(doc, remove_digits=True)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        #if stopword_removal:\n",
        "        doc = remove_stopwords(doc, is_lower_case=True)\n",
        "        normalized_corpus.append(doc)\n",
        "    return normalized_corpus\n",
        "  \n",
        "trainDF['title']=normalize_corpus(trainDF['title'])\n",
        "test['title']=normalize_corpus(test['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Fxe723cPOZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#stratified sampling\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, random_state=25, test_size=0.1)\n",
        "\n",
        "\n",
        "for train_index, valid_index in sss.split(X=trainDF.index.values, y=trainDF['Category'].values):\n",
        "    train_y, valid_y = trainDF.iloc[train_index]['Category'], trainDF.iloc[valid_index]['Category']\n",
        "    train_x, valid_x = trainDF.iloc[train_index]['title'], trainDF.iloc[valid_index]['title']\n",
        "\n",
        "train_y= train_y.reset_index(drop=True)\n",
        "train_x= train_x.reset_index(drop=True)\n",
        "valid_y= valid_y.reset_index(drop=True)\n",
        "valid_x= valid_x.reset_index(drop=True)\n",
        "\n",
        "print(valid_x.shape)\n",
        "print(train_x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzWkzMqVPZC8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['title'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "xtest_tfidf = tfidf_vect.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUOQUdzXPcQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=1000)\n",
        "tfidf_vect_ngram.fit(trainDF['title'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "xtest_tfidf = tfidf_vect_ngram.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QIWNKSaHPeAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['title'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3UNVa-aPfuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_textual(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)  \n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)\n",
        "  \n",
        "\n",
        "def score_model(classifier, feature_vector_train, label, feature_vector_test,results):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    #predict one by one  and write label to test set\n",
        "    predictions = classifier.predict(feature_vector_test)\n",
        "    \n",
        "    return predictions\n",
        "  \n",
        "def generate_result(prediction,results):\n",
        "    for i in range (0,len(test_x)):\n",
        "      results= results.append ({'itemid':test['itemid'][i], 'Category': predictions[i]}, ignore_index=True)\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2CeHGPaPh7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model_textual(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"LR, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EL9t7svTPkZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train on linear model\n",
        "predictions= score_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf,results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYa6TjGtPnrT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result=generate_result(predictions,results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btuiYdSHPq-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result.to_csv('C:/Users/alyta/Downloads/year 2/year 2 sem 2/NDSC/dataset/predictions_linear.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKouWdGePvqF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Visual Model"
      ]
    },
    {
      "metadata": {
        "id": "mN7nZuKTPyvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/gdrive/'My Drive'/Data/mobile_image.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kv0zmqQXQEC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/gdrive/'My Drive'/Data/fashion_image.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPKfx_80QFyr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/gdrive/'My Drive'/Data/beauty_image.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoBV7iC_QIPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjfMJacnQImX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import PIL\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUr0KhYmQIoo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZxyKoWJQIrl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spIwnX45QItm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import math\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from sklearn.metrics import log_loss\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_k9-YCgQWgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Define necessary functions"
      ]
    },
    {
      "metadata": {
        "id": "NBX6A4CuQI6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function specifies the transformation performed on the validation set\n",
        "def get_test_time_transform(scale_size, crop_size):\n",
        "\ttest_time_transform = torchvision.transforms.Compose([\n",
        "\t\ttorchvision.transforms.Resize(scale_size),\n",
        "\t\ttorchvision.transforms.CenterCrop(crop_size),\n",
        "\t\ttorchvision.transforms.ToTensor(),\n",
        "\t\ttorchvision.transforms.Normalize(\n",
        "\t\t\tmean=[0.485, 0.456, 0.406],\n",
        "\t\t\tstd=[0.229, 0.224, 0.225]\n",
        "\t\t)\n",
        "\t])\n",
        "\n",
        "\treturn test_time_transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjqJQJIdQI9C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function specifies the transformation performed on the train set\n",
        "def get_train_time_transform_simple(scale_size, crop_size):\n",
        "\ttrain_time_transform = torchvision.transforms.Compose([\n",
        "\t\ttorchvision.transforms.Resize(scale_size),\n",
        "\t\ttorchvision.transforms.RandomCrop(crop_size),\n",
        "\t\ttorchvision.transforms.RandomHorizontalFlip(),\n",
        "\t\ttorchvision.transforms.ToTensor(),\n",
        "\t\ttorchvision.transforms.Normalize(\n",
        "\t\t\tmean=[0.485, 0.456, 0.406],\n",
        "\t\t\tstd=[0.229, 0.224, 0.225]\n",
        "\t\t)\n",
        "\t])\n",
        "\n",
        "\treturn train_time_transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYLh6rccQI_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recover_image(im, writer=False):\n",
        "\tmean = np.array([0.485, 0.456, 0.406])\n",
        "\tstd = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\tim = im.numpy().transpose((1, 2, 0))\n",
        "\tim = std * im + mean\n",
        "\tim = np.clip(im, 0, 1)\n",
        "\n",
        "\tif writer:\n",
        "\t\tim = im.transpose((2, 0, 1))\n",
        "\n",
        "\treturn im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3YNHwlUQksg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function trains the visual model\n",
        "def train_model_visual(model, device, dataloaders, criterion, optimizer, scheduler=None, num_epochs=25, in_notebook=True):\n",
        "  since = time.time()\n",
        "  # initialize a progress bar\n",
        "  tqdm_func = tqdm_notebook if in_notebook else tqdm\n",
        "  writer = SummaryWriter()\n",
        "\n",
        "  # initialize the best model\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_log_loss = math.inf\n",
        "\n",
        "  # initialize a list for storing the performance history\n",
        "  train_performance_history = []\n",
        "  val_performance_history = []\n",
        "\n",
        "  for epoch in tqdm_func(range(num_epochs)):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        # set model to training mode\n",
        "        if scheduler is not None:\n",
        "          scheduler.step()\n",
        "        model.train()\n",
        "        num_batch_per_epoch = int(\n",
        "          np.ceil(len(dataloaders['train'].dataset) * 1.0 / dataloaders['train'].batch_size))\n",
        "      else:\n",
        "        # set model to evaluation mode\n",
        "        model.eval()\n",
        "        num_batch_per_epoch = int(\n",
        "          np.ceil(len(dataloaders['val'].dataset) * 1.0 / dataloaders['val'].batch_size))\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for batch_idx, batch_data in tqdm_func(enumerate(dataloaders[phase]), total=num_batch_per_epoch):\n",
        "\n",
        "        writer_step = num_batch_per_epoch * epoch + (batch_idx + 1)\n",
        "\n",
        "        inputs = batch_data['image'].to(device)\n",
        "        labels = batch_data['category'].to(device)\n",
        "        img = recover_image(inputs[0].cpu(), writer=True)\n",
        "\n",
        "        writer.add_image('image/transform', img, writer_step)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          # Get model outputs and calculate loss\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            writer.add_scalar('loss/loss', loss.item(), writer_step)\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = (running_corrects.double() / len(dataloaders[phase].dataset)).item()\n",
        "      print('Epoch {} {} Loss: {:.4f} Acc: {:.4f}'.format(epoch, phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      # record the history\n",
        "      train_performance_history.append({\n",
        "        'epoch': epoch,\n",
        "        'train_loss': epoch_loss,\n",
        "        'train_acc': epoch_acc\n",
        "      })\n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val':\n",
        "        val_performance_history.append({\n",
        "          'epoch': epoch,\n",
        "          'val_loss': epoch_loss,\n",
        "          'val_acc': epoch_acc\n",
        "        })\n",
        "\n",
        "        writer.add_scalar('loss/val_loss', epoch_loss, writer_step)\n",
        "        writer.add_scalar('accuracy/val_acc', epoch_acc, writer_step)\n",
        "        if epoch_loss < best_log_loss:\n",
        "          best_log_loss = epoch_loss\n",
        "          best_acc = epoch_acc\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      else:\n",
        "        writer.add_scalar('accuracy/train_acc', epoch_acc, writer_step)\n",
        "\n",
        "    print()\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best validation Loss: {:4f} Acc: {:.4f}'.format(best_log_loss, best_acc))\n",
        "\n",
        "  # process the full history\n",
        "  full_performance_history = pd.DataFrame(train_performance_history).merge(\n",
        "    pd.DataFrame(val_performance_history), on='epoch', how='left')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  writer.close()\n",
        "  return model, full_performance_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86YokMAVQkyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function freezes all the gradients of the model if transfer learning is used\n",
        "def set_parameter_requires_grad(model, feature_extract):\n",
        "\tif feature_extract:\n",
        "\t\tfor param in model.parameters():\n",
        "\t\t\tparam.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wOsRiufRQk0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function initializes the model based on the name passed to it, only resnet is tested\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "\tmodel_ft = None\n",
        "\n",
        "\tif 'resnet' in model_name:\n",
        "\n",
        "\t\tresnet_model_mapping = {\n",
        "\t\t\t'resnet18': models.resnet18(pretrained=use_pretrained),\n",
        "\t\t\t'resnet34': models.resnet34(pretrained=use_pretrained),\n",
        "\t\t\t'resnet50': models.resnet50(pretrained=use_pretrained),\n",
        "\t\t\t'resnet101': models.resnet101(pretrained=use_pretrained),\n",
        "\t\t\t'resnet152': models.resnet152(pretrained=use_pretrained)\n",
        "\t\t}\n",
        "\n",
        "\t\tmodel_ft = resnet_model_mapping[model_name]\n",
        "\t\tset_parameter_requires_grad(model_ft, feature_extract)\n",
        "\n",
        "\t\tnum_ftrs = model_ft.fc.in_features\n",
        "\t\tmodel_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\telse:\n",
        "\t\tprint(\"Invalid model name, exiting...\")\n",
        "\t\texit()\n",
        "\n",
        "\treturn model_ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ij-Ph8cOQk3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function is to undersample categories with large proportion of data\n",
        "def undersample(category, sample_size, sample_df):\n",
        "  cat_under = sample_df[sample_df['Category'] == category]\n",
        "  return cat_under.sample(sample_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BcvTBmKgQk5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function is to oversample categories with too little data\n",
        "def oversample(category, sample_size, sample_df):\n",
        "  cat_over = sample_df[sample_df['Category'] == category]\n",
        "  return cat_over.sample(sample_size, replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jkYDkztRO43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Define neccessary class"
      ]
    },
    {
      "metadata": {
        "id": "3lU9yX2zQlBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProductCategoryDataset(Dataset):\n",
        "  \"\"\"Product category dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, csv_file, root_dir, transform=None, train=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      csv_file (string): Path to the csv file with annotations.\n",
        "      root_dir (string): Directory with all the images.\n",
        "      transform (callable, optional): Optional transform to be applied\n",
        "        on a sample.\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #Some image_path do not have .jpg which will lead to error\n",
        "    image_path = self.data.iloc[idx]['image_path']\n",
        "    if(image_path[-3:] != \"jpg\"):\n",
        "      image_path += \".jpg\"\n",
        "    im_path = os.path.join(self.root_dir, image_path)\n",
        "    image = Image.open(im_path).convert('RGB')\n",
        "\n",
        "    if self.train:\n",
        "      data_sample = {\n",
        "        'image': image,\n",
        "        'id': self.data.iloc[idx]['itemid'],\n",
        "        'category': self.data.iloc[idx]['Category']\n",
        "      }\n",
        "    else:\n",
        "      data_sample = {\n",
        "        'image': image,\n",
        "        'id': self.data.iloc[idx]['itemid']\n",
        "      }\n",
        "\n",
        "    if self.transform:\n",
        "      data_sample['image'] = self.transform(image)\n",
        "\n",
        "    return data_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2a9Rvgy2SGGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Explore Train Data"
      ]
    },
    {
      "metadata": {
        "id": "abk2ROGNSIU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_csv('/content/gdrive/My Drive/Data/train.csv')\n",
        "print(train_labels.shape)\n",
        "display(train_labels.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aof7X9amSKEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category_count = train_labels.groupby('Category', as_index=False).agg({'itemid': 'count'}).rename(\n",
        "    columns={'itemid': 'count'}).sort_values('count', ascending=False).reset_index(drop=False)\n",
        "display(category_count.head())\n",
        "print(category_count.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWZNjNkQSKGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Display total number of data by category\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(16, 8))\n",
        "_ = sns.barplot(x=np.arange(category_count.shape[0]), y=category_count['count'].values, color='lightblue')\n",
        "_ = plt.xticks(range(category_count.shape[0]), category_count['Category'].values, rotation=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OR8FnEkLSKI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category_count_asc = train_labels.groupby('Category', as_index=False).agg({'itemid': 'count'}).rename(\n",
        "    columns={'itemid': 'count'}).sort_values('count', ascending=True).reset_index(drop=False)\n",
        "display(category_count_asc.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QjFRVZ2-SRdi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Under sampling of majority categories and oversampling of minority categories"
      ]
    },
    {
      "metadata": {
        "id": "28Zecd4YSKNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_0_under = undersample(0, 400, train_labels)\n",
        "cat_1_under = undersample(1, 400, train_labels)\n",
        "cat_2_under = undersample(2, 400, train_labels)\n",
        "cat_3_under = undersample(3, 400, train_labels)\n",
        "cat_4_under = undersample(4, 400, train_labels)\n",
        "cat_5_under = undersample(5, 400, train_labels)\n",
        "cat_6_under = undersample(6, 400, train_labels)\n",
        "cat_7_under = undersample(7, 400, train_labels)\n",
        "cat_8_under = undersample(8, 400, train_labels)\n",
        "cat_9_under = undersample(9, 400, train_labels)\n",
        "cat_10_under = undersample(10, 400, train_labels)\n",
        "cat_11_under = undersample(11, 400, train_labels)\n",
        "cat_12_under = undersample(12, 400, train_labels)\n",
        "cat_13_under = undersample(13, 400, train_labels)\n",
        "cat_14_under = undersample(14, 400, train_labels)\n",
        "cat_15_under = undersample(15, 400, train_labels)\n",
        "cat_16_under = undersample(16, 400, train_labels)\n",
        "cat_17_under = undersample(17, 400, train_labels)\n",
        "cat_18_under = undersample(18, 400, train_labels)\n",
        "cat_19_under = undersample(19, 400, train_labels)\n",
        "cat_20_under = undersample(20, 400, train_labels)\n",
        "cat_21_under = undersample(21, 400, train_labels)\n",
        "cat_22_under = undersample(22, 400, train_labels)\n",
        "cat_23_under = undersample(23, 400, train_labels)\n",
        "cat_24_under = undersample(24, 400, train_labels)\n",
        "cat_25_under = undersample(25, 400, train_labels)\n",
        "cat_26_under = undersample(26, 400, train_labels)\n",
        "cat_27_under = undersample(27, 400, train_labels)\n",
        "cat_28_under = undersample(28, 400, train_labels)\n",
        "cat_29_under = undersample(29, 400, train_labels)\n",
        "cat_30_under = undersample(30, 400, train_labels)\n",
        "cat_31_under = undersample(31, 400, train_labels)\n",
        "cat_32_under = undersample(32, 400, train_labels)\n",
        "cat_33_under = undersample(33, 400, train_labels)\n",
        "cat_34_under = undersample(34, 400, train_labels)\n",
        "cat_35_under = undersample(35, 400, train_labels)\n",
        "cat_36_under = undersample(36, 400, train_labels)\n",
        "cat_37_under = undersample(37, 400, train_labels)\n",
        "cat_38_under = undersample(38, 400, train_labels)\n",
        "cat_39_under = undersample(39, 400, train_labels)\n",
        "cat_41_under = undersample(41, 400, train_labels)\n",
        "cat_42_under = undersample(42, 400, train_labels)\n",
        "cat_43_under = undersample(43, 400, train_labels)\n",
        "cat_44_under = undersample(44, 400, train_labels)\n",
        "cat_45_under = undersample(45, 400, train_labels)\n",
        "cat_46_under = undersample(46, 400, train_labels)\n",
        "cat_47_under = undersample(47, 400, train_labels)\n",
        "cat_48_under = undersample(48, 400, train_labels)\n",
        "cat_49_under = undersample(49, 400, train_labels)\n",
        "cat_51_under = undersample(51, 400, train_labels)\n",
        "cat_53_under = undersample(53, 400, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmvXGlyPSKR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_40_over = oversample(40, 400, train_labels)\n",
        "cat_54_over = oversample(54, 400, train_labels)\n",
        "cat_50_over = oversample(50, 400, train_labels)\n",
        "cat_56_over = oversample(56, 400, train_labels)\n",
        "cat_55_over = oversample(55, 400, train_labels)\n",
        "cat_52_over = oversample(52, 400, train_labels)\n",
        "cat_57_over = oversample(57, 400, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDSfTgvASKUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_balanced = pd.concat([cat_0_under, cat_1_under, cat_2_under, cat_3_under, cat_4_under, cat_5_under, cat_6_under, cat_7_under, cat_8_under, cat_9_under\\\n",
        "                           , cat_10_under, cat_11_under, cat_12_under, cat_13_under, cat_14_under, cat_15_under, cat_16_under, cat_17_under, cat_18_under, cat_19_under\\\n",
        "                           , cat_20_under, cat_21_under, cat_22_under, cat_23_under, cat_24_under, cat_25_under, cat_26_under, cat_27_under, cat_28_under, cat_29_under\\\n",
        "                           , cat_30_under, cat_31_under, cat_32_under, cat_33_under, cat_34_under, cat_35_under, cat_36_under, cat_37_under, cat_38_under, cat_39_under\\\n",
        "                           , cat_40_over, cat_41_under, cat_42_under, cat_43_under, cat_44_under, cat_45_under, cat_46_under, cat_47_under, cat_48_under, cat_49_under\\\n",
        "                           , cat_50_over, cat_51_under, cat_52_over, cat_53_under, cat_54_over, cat_55_over, cat_56_over, cat_57_over])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AJICcdYuSKXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_balanced.shape)\n",
        "display(train_balanced.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPpRBDunSKaE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category_count = train_balanced.groupby('Category', as_index=False).agg({'itemid': 'count'}).rename(\n",
        "    columns={'itemid': 'count'}).sort_values('count', ascending=False).reset_index(drop=False)\n",
        "display(category_count.head())\n",
        "print(category_count.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAFqpzB_SKcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(16, 8))\n",
        "_ = sns.barplot(x=np.arange(category_count.shape[0]), y=category_count['count'].values, color='lightblue')\n",
        "_ = plt.xticks(range(category_count.shape[0]), category_count['Category'].values, rotation=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61DqVUduTFox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "k5MRdh8mSKhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2qAzAucTX6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Stratified sampling for train and validation set"
      ]
    },
    {
      "metadata": {
        "id": "-S0qyvxHSKfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, random_state=25, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IGoURPG4SKLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_balanced = train_balanced.reset_index(drop=True)\n",
        "\n",
        "for train_index, valid_index in sss.split(X=train_balanced.index.values, y=train_balanced['Category'].values):\n",
        "    train_train_df, train_valid_df = train_balanced.iloc[train_index], train_balanced.iloc[valid_index]\n",
        "    \n",
        "train_train_df = train_train_df.reset_index(drop=True)\n",
        "train_valid_df = train_valid_df.reset_index(drop=True)\n",
        "\n",
        "print(train_train_df.shape)\n",
        "print(train_valid_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "StilIqB1TgMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "sns.countplot(x=train_train_df['Category'].values, order=range(58), color='lightblue')\n",
        "_ = plt.xticks([])\n",
        "_ = plt.title('train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCC5En3zTiQP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "sns.countplot(train_valid_df['Category'].values, order=range(58), color='lightgreen')\n",
        "_ = plt.xticks([])\n",
        "_ = plt.title('validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01hg5-ugTiS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_train_df.to_csv('/content/gdrive/My Drive/Data/processed_data/train_train_dataset.csv', index=False)\n",
        "train_valid_df.to_csv('/content/gdrive/My Drive/Data/processed_data/train_valid_dataset.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NKsJnNyOTmJi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Train Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "5MoiYnEMTiaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set the path and directory for train and validation\n",
        "train_dataset_csv = '/content/gdrive/My Drive/Data/processed_data/train_train_dataset.csv'\n",
        "val_dataset_csv = '/content/gdrive/My Drive/Data/processed_data/train_valid_dataset.csv'\n",
        "train_root_dir = '/content'\n",
        "test_root_dir = '/content'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTq_DBeOTif2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'resnet152'\n",
        "input_size = 224\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 58\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 10\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "# when True we only update the reshaped layer params\n",
        "# Here, it is set as True as we are making use of transfer learning\n",
        "feature_extract = True\n",
        "\n",
        "crop_size = input_size\n",
        "# we use (scale_size: 256, crop_size: 224)\n",
        "# for resize and cropping\n",
        "scale_size = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-NtcapQTiqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# transforms settings\n",
        "train_transform_simple = get_train_time_transform_simple(scale_size=scale_size, crop_size=crop_size)\n",
        "test_transform = get_test_time_transform(scale_size=crop_size, crop_size=crop_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7yclKAOTiwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# datasets with transforms\n",
        "train_dataset_simple = ProductCategoryDataset(\n",
        "    csv_file=train_dataset_csv, root_dir=train_root_dir, transform=train_transform_simple)\n",
        "val_dataset = ProductCategoryDataset(\n",
        "    csv_file=val_dataset_csv, root_dir=train_root_dir, transform=test_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjuvCHxMTi5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "train_data_loader_simple = DataLoader(\n",
        "    train_dataset_simple, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# data loaders in dict\n",
        "data_loaders_simple = {\n",
        "    'train': train_data_loader_simple,\n",
        "    'val': val_data_loader\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrxIY0U0Titn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('you are using device: ', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRGKxdUgX4O0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Initialize Model"
      ]
    },
    {
      "metadata": {
        "id": "TthmX7ZCTilM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the model for this run\n",
        "model_ft = initialize_model(\n",
        "    model_name=model_name, num_classes=num_classes, feature_extract=feature_extract, \n",
        "    use_pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6eK45PT-Tiit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4T33OWiTidq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method for transfer learning, we will only update \n",
        "#  the parameters that we have just initialized, i.e. the parameters with \n",
        "#  requires_grad is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbwwaROETiYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define loss type\n",
        "logloss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Only parameters of final layer are being optimized as we are making use of transfer learning\n",
        "adam_optimizer = torch.optim.Adam(params_to_update, lr=0.0005, weight_decay=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvH9iSSWYDUA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training"
      ]
    },
    {
      "metadata": {
        "id": "aHTGc0yFTiVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model training\n",
        "trained_model, full_performance_history = train_model_visual(\n",
        "    model=model_ft, device=device, dataloaders=data_loaders_simple, \n",
        "    criterion=logloss_criterion, optimizer=adam_optimizer, scheduler=None, \n",
        "    num_epochs=num_epochs, in_notebook=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0OLZgaJYLRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(trained_model.state_dict(), '/content/gdrive/My Drive/Data/model/resnet152_fe_adam_10epoch_simple_0.0005.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUumleGzYM2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display(full_performance_history.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKrzYkzLY6RJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Prepare Model for Prediction"
      ]
    },
    {
      "metadata": {
        "id": "4BJ0xDOfZBq5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the model for this run\n",
        "model = initialize_model(\n",
        "    model_name=model_name, num_classes=num_classes, feature_extract=feature_extract, \n",
        "    use_pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8YdQbiyZEWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/Data/model/resnet152_fe_adam_10epoch_simple_0.0005.pt'))\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFclXxwe1V31",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Explore sample submission"
      ]
    },
    {
      "metadata": {
        "id": "LSZ5Y4Tp1YiI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('/content/gdrive/My Drive/Data/data_info_val_sample_submission.csv')\n",
        "\n",
        "print(sample_submission.shape)\n",
        "display(sample_submission.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NR8VT6FBzkQg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Explore Test Data Set"
      ]
    },
    {
      "metadata": {
        "id": "C8ZWaVO8zmlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_labels = pd.read_csv('/content/gdrive/My Drive/Data/test.csv')\n",
        "print(test_labels.shape)\n",
        "display(test_labels.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jKX4TQAz5F2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_samples = test_labels.sample(16)\n",
        "display(test_samples.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UioV_E63z6QF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Prepare Test Data Set"
      ]
    },
    {
      "metadata": {
        "id": "v9yTerNhz8KJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
        "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                              std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvHixLtLZQWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ]
    },
    {
      "metadata": {
        "id": "m-Xc8vxSZRsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reset results to accumulate predictions\n",
        "results = pd.DataFrame(columns = [\"itemid\", \"Category\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuOKzBZbZT-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, len(test_labels.index)):\n",
        "  image_path = test_labels['image_path'][i]\n",
        "  if(image_path[-3:] != \"jpg\"):\n",
        "      image_path += \".jpg\"\n",
        "  img = Image.open('/content/' + image_path)\n",
        "  img = transform_pipeline(img)\n",
        "  img = img.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n",
        "  prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
        "  prediction = prediction.data.numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
        "  results = results.append({'itemid': test_labels['itemid'][i], 'Category': prediction}, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JqcIZkk9Zaim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results.to_csv('/content/gdrive/My Drive/Data/predictions.csv', index = false)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eaYhBzyDZuwm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Text with Visual Model"
      ]
    },
    {
      "metadata": {
        "id": "Qma7ghQfZ7AA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reset results to accumulate predictions\n",
        "results = pd.DataFrame(columns = [\"itemid\", \"Category\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rqc-ZpakZ9Jp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nltk.download('stopwords')\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "\n",
        "#use porter stemmer\n",
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "  \n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "#remove digits\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def normalize_corpus(corpus):\n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        #if text_lemmatization:\n",
        "         #   doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        #if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "        doc=simple_stemmer(doc)\n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        doc = remove_special_characters(doc, remove_digits=True)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        #if stopword_removal:\n",
        "        doc = remove_stopwords(doc, is_lower_case=True) \n",
        "        normalized_corpus.append(doc)\n",
        "    return normalized_corpus\n",
        "  \n",
        "trainDF['title']=normalize_corpus(trainDF['title'])\n",
        "test['title']=normalize_corpus(test['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VtHYrXxjaCKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#splilt into 3 categories\n",
        "beauty = trainDF[trainDF[\"Category\"] <= 16]\n",
        "fashion = trainDF[trainDF[\"Category\"] >= 17]\n",
        "fashion = fashion[fashion[\"Category\"] <= 30]\n",
        "mobile = trainDF[trainDF[\"Category\"] >= 31]\n",
        "print((len(beauty.index) + len(fashion.index) + len(mobile.index)) == len(trainDF.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02-TNf2RaENK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "beauty_x=beauty['title']\n",
        "beauty_y=beauty['Category']\n",
        "\n",
        "fashion_x=fashion['title']\n",
        "fashion_y=fashion['Category']\n",
        "\n",
        "mobile_x=mobile['title']\n",
        "mobile_y=mobile['Category']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMVBjj5ZaGsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf for beauty, fashion and mobile\n",
        "\n",
        "#beauty\n",
        "beauty_tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "beauty_tfidf_vect.fit(beauty['title'])\n",
        "beauty_tfidf =  beauty_tfidf_vect.transform(beauty_x)\n",
        "\n",
        "#fashion\n",
        "fashion_tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "fashion_tfidf_vect.fit(fashion['title'])\n",
        "fashion_tfidf =  fashion_tfidf_vect.transform(fashion_x)\n",
        "\n",
        "#mobile\n",
        "mobile_tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "mobile_tfidf_vect.fit(mobile['title'])\n",
        "mobile_tfidf =  mobile_tfidf_vect.transform(mobile_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7BL0ey7aIVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#predict one by one\n",
        "def train_model(classifier, feature_vector_train, label):\n",
        "    # fit the training dataset on the classifier\n",
        "    model=classifier.fit(feature_vector_train, label)\n",
        "    return model\n",
        "\n",
        "def predict(model,title):\n",
        "    prediction= model.predict(title)\n",
        "    return prediction;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOu_GRCjaJ-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train model for 3 cat:\n",
        "beauty_model=train_model(linear_model.LogisticRegression(),beauty_tfidf , beauty_y)\n",
        "fashion_model=train_model(linear_model.LogisticRegression(),fashion_tfidf, fashion_y)\n",
        "mobile_model=train_model(linear_model.LogisticRegression(),mobile_tfidf, mobile_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8-qVwPeaOVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visual_predictions = pd.read_csv('/content/gdrive/My Drive/Data/predictions.csv')\n",
        "\n",
        "for i in range(0, len(visual_predictions.index)):\n",
        "  if(visual_predictions[\"Category\"][i] <= 16):\n",
        "    #beauty\n",
        "    title = test[\"title\"][i]\n",
        "    test_beauty_tfidf= beauty_tfidf_vect.transform([title])\n",
        "    prediction=int(predict(beauty_model,test_beauty_tfidf))\n",
        "    results= results.append ({'itemid':test['itemid'][i], 'Category': prediction}, ignore_index=True)\n",
        "    \n",
        "  elif(17 <= visual_predictions[\"Category\"][i] <= 30):\n",
        "    #fashion\n",
        "    title = test[\"title\"][i]\n",
        "    test_fashion_tfidf= fashion_tfidf_vect.transform([title])\n",
        "    prediction=int(predict(fashion_model,test_fashion_tfidf))\n",
        "    results= results.append ({'itemid':test['itemid'][i], 'Category': prediction}, ignore_index=True)\n",
        "    \n",
        "  else:\n",
        "    title = test[\"title\"][i]\n",
        "    #mobile\n",
        "    test_mobile_tfidf= mobile_tfidf_vect.transform([title])\n",
        "    prediction=int(predict(mobile_model,test_mobile_tfidf))\n",
        "    results= results.append ({'itemid':test['itemid'][i], 'Category': prediction}, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ljh39X6aRRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results.to_csv(''C:/Users/alyta/Downloads/year 2/year 2 sem 2/NDSC/dataset/predictions_new.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}